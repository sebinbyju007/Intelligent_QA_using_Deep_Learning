{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_program.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi7klliUoq5H",
        "outputId": "a4597778-0eea-4479-8741-c323deb8ed22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpYdZ2_CpAc8",
        "outputId": "30914ecd-0437-473b-ac1e-5e8e1c88b807"
      },
      "source": [
        "!pip install distance\n",
        "!pip install fuzzywuzzy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting distance\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 27.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61kB 23.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81kB 22.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 92kB 24.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 122kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 133kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 143kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 153kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 174kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 23.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-cp37-none-any.whl size=16261 sha256=bcb3cae5b8b4c624037cf28ea8e1a191c843d49af646275be9d9cf55d342a15d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "Successfully built distance\n",
            "Installing collected packages: distance\n",
            "Successfully installed distance-0.1.3\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqN-5mdLW3O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80187da-47c1-45d2-b1ae-ce4cf8b054dc"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle \n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Project')\n",
        "import feature_engineering"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIWzV0UNpU8b"
      },
      "source": [
        "class Test:\n",
        "  def __init__(self,tokenizer_name = '/content/drive/MyDrive/Project/tokenizer.pickle',ques_dict_name = '/content/drive/MyDrive/Project/q_dict.pickle',\\\n",
        "               ss_name = '/content/drive/MyDrive/Project/ss.pickle',model_name = '/content/drive/MyDrive/Project/royal_model.h5'):\n",
        "    \n",
        "    with open(tokenizer_name, 'rb') as handle:\n",
        "      self.tokenizer = pickle.load(handle)\n",
        "    self.word_index = self.tokenizer.word_index\n",
        "    print('Tokenizer Loaded')\n",
        "\n",
        "    with open(ques_dict_name, 'rb') as handle:\n",
        "      self.ques_dict = pickle.load(handle)\n",
        "    print('Question Dict Loaded')\n",
        "\n",
        "    with open(ss_name, 'rb') as handle:\n",
        "      self.ss = pickle.load(handle)\n",
        "    print('Standard Scalar Loaded') \n",
        "\n",
        "    self.model = load_model(model_name)\n",
        "    print('Model Loaded')\n",
        "\n",
        "    self.test_question1 = []\n",
        "    self.test_question2 = []\n",
        "    self.featureEngineering = feature_engineering.FeatureEngineering()\n",
        "\n",
        "  def text_to_wordlist(self,text):\n",
        "    \n",
        "    text = text.lower().split()\n",
        "    text = \" \".join(text)\n",
        "    text = re.sub(r\"coronavirus\",\"corona virus 2019\",text)\n",
        "    text = re.sub(r\"covid-19\",\"corona virus 2019\",text)\n",
        "    text = re.sub(r\"covid - 19\",\"corona virus 2019\",text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    return text\n",
        "  \n",
        "  def tokenization(self,df):\n",
        "\n",
        "    for text in df.question1.values:\n",
        "      self.test_question1.append(self.text_to_wordlist(text))\n",
        "      \n",
        "    for text in df.question2.values:\n",
        "      self.test_question2.append(self.text_to_wordlist(text))\n",
        "\n",
        "    self.test_question1 = self.tokenizer.texts_to_sequences(self.test_question1)  \n",
        "    self.test_question2 = self.tokenizer.texts_to_sequences(self.test_question2)\n",
        "\n",
        "    print('Tokenization Done....')\n",
        "\n",
        "    MAX_SEQUENCE_LENGTH = 60  \n",
        "    self.test_question1 = pad_sequences(self.test_question1, maxlen=MAX_SEQUENCE_LENGTH)  \n",
        "    self.test_question2 = pad_sequences(self.test_question2, maxlen=MAX_SEQUENCE_LENGTH)  \n",
        "    print('Shape of test data vtensor:', self.test_question1.shape)\n",
        "\n",
        "  def feature_extraction(self,df):\n",
        "    questions = pd.concat([df[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
        "    for i in range(questions.shape[0]):\n",
        "            self.ques_dict[questions.question1[i]].add(questions.question2[i])\n",
        "            self.ques_dict[questions.question2[i]].add(questions.question1[i])\n",
        "    df['q1_q2_intersect'] = df.apply(lambda row: len(set(self.ques_dict.get(row[1])).intersection(set(self.ques_dict.get(row[2])))), axis=1, raw=True)\n",
        "    df['q1_freq'] = df.apply(lambda row: len(self.ques_dict.get(row[1])), axis=1, raw=True)\n",
        "    df['q2_freq'] = df.apply(lambda row: len(self.ques_dict.get(row[2])), axis=1, raw=True)\n",
        "\n",
        "    print('Feature Extraction Completed..')\n",
        "    return df\n",
        "\n",
        "  def leak_calculator(self,df,df_nlp):\n",
        "    df_nlp = df_nlp.fillna(0)\n",
        "    df['test_id'] = df_nlp['test_id']\n",
        "\n",
        "    df = df.merge(df_nlp,on='test_id',how='left')\n",
        "    leak = df.drop(['test_id','question1','question2'], axis=1)\n",
        "    leak = self.ss.transform(leak)\n",
        "    \n",
        "    print('Leak Data Completed..')\n",
        "    return leak\n",
        "\n",
        "\n",
        "  def predict(self,question1,question2):\n",
        "    self.test_question1 = []\n",
        "    self.test_question2 = []\n",
        "    df = pd.DataFrame(data=[[0,question1,question2]] , columns=['test_id','question1','question2'])\n",
        "    self.tokenization(df)\n",
        "    df = self.feature_extraction(df)\n",
        "    df_nlp = self.featureEngineering.two_question(question1,question2)\n",
        "    leaks = self.leak_calculator(df,df_nlp)\n",
        "    preds = self.model.predict([self.test_question1, self.test_question2, leaks], batch_size=8192, verbose=1,)\n",
        "    preds += self.model.predict([self.test_question2, self.test_question1, leaks], batch_size=8192, verbose=1)\n",
        "    return preds[0][0]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jViNFuCKpXou",
        "outputId": "745b347a-7c3b-441b-c7a0-fa63c5858b52"
      },
      "source": [
        "test = Test() # run only once"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer Loaded\n",
            "Question Dict Loaded\n",
            "Standard Scalar Loaded\n",
            "Model Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzA2-Ud8ppfd"
      },
      "source": [
        "question1 = 'What will be the long term impact of demonetization of rs 500 and rs 1000?'\n",
        "question2 = 'What can be poker the reasons for discontinuation of 500 and 1000 rupee?'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5vPeB80pyQe",
        "outputId": "658b2be4-6da9-4546-c21c-0efc9cb3e041"
      },
      "source": [
        "print('Predicted probability:- ',test.predict(question1=question1 , question2=question2))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenization Done....\n",
            "Shape of test data vtensor: (1, 60)\n",
            "Feature Extraction Completed..\n",
            "Extracting Token Features...\n",
            "Extracting Fuzzy Features..\n",
            "Extracting Distance Features..\n",
            "- word_mover_dis done...\n",
            "- embedding done...\n",
            "- spatial distance done\n",
            "Leak Data Completed..\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "Predicted probability:-  0.068573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpDtV73op6DJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}