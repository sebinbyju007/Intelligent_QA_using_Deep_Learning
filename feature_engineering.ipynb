{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature engineering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHTdSMPaH8WV",
        "outputId": "95b88638-1d7f-4d7c-ba22-d6e2dc7d52ab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO4L0qlHIIod",
        "outputId": "ea4ba800-20b0-43a5-de13-32d5baf385b0"
      },
      "source": [
        "!pip install distance\n",
        "!pip install fuzzywuzzy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting distance\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 12.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 122kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 133kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 143kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 153kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 174kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 5.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-cp37-none-any.whl size=16261 sha256=bf4768650653d425b6b5bd22e7a2e5f3518915b700e48cfcffc4511f37e6ff1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "Successfully built distance\n",
            "Installing collected packages: distance\n",
            "Successfully installed distance-0.1.3\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWH4RzGZIFnR",
        "outputId": "1162a896-5ae1-4f92-818a-633596e83ae7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import distance\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from fuzzywuzzy import fuzz\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "from scipy.spatial.distance import cosine, cityblock, canberra, euclidean, minkowski\n",
        "import gensim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDvqhZFPH8_G"
      },
      "source": [
        "\n",
        "class FeatureEngineering:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.glove_model = []\n",
        "        self.SAFE_DIV = 0.0001\n",
        "        with open('/content/drive/MyDrive/Project/glove_model.pickle', 'rb') as handle:\n",
        "            self.glove_model = pickle.load(handle)\n",
        "        self.STOP_WORDS = stopwords.words('english')\n",
        "\n",
        "    def share_word_normalization(self, data):\n",
        "        first_word = set(map(lambda word: word.lower().strip(), data['question1'].split(\" \")))\n",
        "        second_word = set(map(lambda word: word.lower().strip(), data['question2'].split(\" \")))\n",
        "        return 1.0 * len(first_word & second_word) / (len(first_word) + len(second_word))\n",
        "\n",
        "    def two_question(self, question1, question2):\n",
        "        df = pd.DataFrame(data=[[0, question1, question2]], columns=['test_id', 'question1', 'question2'])\n",
        "        df_tm = self.text_mining(df)\n",
        "        df = pd.DataFrame(data=[[0, question1, question2]], columns=['test_id', 'question1', 'question2'])\n",
        "        df_nlp = self.extract_nlp(df)\n",
        "        df_tm = df_tm.merge(df_nlp, on='test_id', how='left')\n",
        "        df_tm = df_tm.drop(['question1', 'question2'], axis=1)\n",
        "        return df_tm\n",
        "\n",
        "    def common_word_normalization(self, data):\n",
        "        first_word = set(map(lambda word: word.lower().strip(), data['question1'].split(\" \")))\n",
        "        second_word = set(map(lambda word: word.lower().strip(), data['question2'].split(\" \")))\n",
        "        return 1.0 * len(first_word & second_word)\n",
        "\n",
        "    def read_csv(self, question2):\n",
        "        df = pd.read_csv('/content/drive/MyDrive/Project/youtube_test_samples.csv', encoding='utf-8')\n",
        "        df['question2'] = [question2] * len(df)\n",
        "        df = df.fillna('')\n",
        "        return df\n",
        "\n",
        "    def total_word_normalization(self, data):\n",
        "        first_word = set(map(lambda word: word.lower().strip(), data['question1'].split(\" \")))\n",
        "        second_word = set(map(lambda word: word.lower().strip(), data['question2'].split(\" \")))\n",
        "        return 1.0 * (len(first_word) + len(second_word))\n",
        "\n",
        "    def get_2_gram_share(self, data):\n",
        "        question1_str = str(data['question1']).lower().split()\n",
        "        question2_str = str(data['question2']).lower().split()\n",
        "        ques1_gram = set([i for i in zip(question1_str, question1_str[1:])])\n",
        "        ques2_gram = set([i for i in zip(question2_str, question2_str[1:])])\n",
        "        shared_gram = ques1_gram.intersection(ques2_gram)\n",
        "        data_gram = 0 if len(ques1_gram) + len(ques2_gram) == 0 else len(shared_gram) / (\n",
        "                len(ques1_gram) + len(ques2_gram))\n",
        "        return data_gram\n",
        "\n",
        "    def text_mining(self, df):\n",
        "        df['ques1_len'] = df['question1'].str.len()\n",
        "        df['ques2_len'] = df['question2'].str.len()\n",
        "        df['len_diff'] = df['ques1_len'] - df['ques2_len']\n",
        "\n",
        "        df['q1_word_len'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "        df['q2_word_len'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
        "        df['words_diff'] = df['q1_word_len'] - df['q2_word_len']\n",
        "\n",
        "        df['q1_caps_count'] = df['question1'].apply(lambda x: sum(1 for i in str(x) if i.isupper()))\n",
        "        df['q2_caps_count'] = df['question2'].apply(lambda x: sum(1 for i in str(x) if i.isupper()))\n",
        "        df['caps_diff'] = df['q1_caps_count'] - df['q2_caps_count']\n",
        "\n",
        "        df['q1_char_len'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
        "        df['q2_char_len'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
        "        df['diff_char_len'] = df['q1_char_len'] - df['q2_char_len']\n",
        "\n",
        "        df['avg_word_len1'] = df['q1_char_len'] / df['q1_word_len']\n",
        "        df['avg_word_len2'] = df['q2_char_len'] / df['q2_word_len']\n",
        "        df['diff_avg_word'] = df['avg_word_len1'] - df['avg_word_len2']\n",
        "\n",
        "        df['common_word'] = df.apply(self.common_word_normalization, axis=1)\n",
        "        df['total_word'] = df.apply(self.total_word_normalization, axis=1)\n",
        "        df['word_share'] = df.apply(self.share_word_normalization, axis=1)\n",
        "        df['share_2_gram'] = df.apply(self.get_2_gram_share, axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def data_preprocess(self, word):\n",
        "        word = str(word).lower()\n",
        "        word = word.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
        "            .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
        "            .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
        "            .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
        "            .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
        "            .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
        "            .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"covid- 19\", \"corona virus 2019\") \\\n",
        "            .replace(\"covid - 19\", \"corona virus 2019\").replace('coronavirus', 'corona virus 2019') \\\n",
        "            .replace('corona virus', 'corona virus 2019')\n",
        "        word = re.sub(r'([0-9]+)000000', r\"\\1m\", word)\n",
        "        word = re.sub(r\"([0-9]+)000\", r\"\\1k\", word)\n",
        "\n",
        "        porter = PorterStemmer()\n",
        "        pattern = re.compile('\\W')\n",
        "\n",
        "        if type('') == type(word):\n",
        "            word = re.sub(pattern, ' ', word)\n",
        "\n",
        "        if type('') == type(word):\n",
        "            word = porter.stem(word)\n",
        "            test = BeautifulSoup(word)\n",
        "            word = test.get_text()\n",
        "\n",
        "        return word\n",
        "\n",
        "    def remove_stop(self, question):\n",
        "        question = str(question)\n",
        "        if question is None or question == np.nan or question == 'NaN':\n",
        "            return ' '\n",
        "\n",
        "        after_stop = [i for i in question.split() if i not in self.STOP_WORDS]\n",
        "        return ' '.join(after_stop)\n",
        "\n",
        "    def word_mover_dis(self, ques1, ques2, model):\n",
        "        ques1 = str(ques1)\n",
        "        ques2 = str(ques2)\n",
        "        ques1 = ques1.split()\n",
        "        ques2 = ques2.split()\n",
        "        return model.wmdistance(ques1, ques2)\n",
        "\n",
        "    def extract_features(self, df):\n",
        "        df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(self.data_preprocess)\n",
        "        df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(self.data_preprocess)\n",
        "\n",
        "        print(\"Extracting Token Features...\")\n",
        "\n",
        "        data_features = df.apply(lambda x: self.get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "\n",
        "        df[\"cwc_min\"] = list(map(lambda x: x[0], data_features))\n",
        "        df[\"cwc_max\"] = list(map(lambda x: x[1], data_features))\n",
        "        df[\"csc_min\"] = list(map(lambda x: x[2], data_features))\n",
        "        df[\"csc_max\"] = list(map(lambda x: x[3], data_features))\n",
        "        df[\"ctc_min\"] = list(map(lambda x: x[4], data_features))\n",
        "        df[\"ctc_max\"] = list(map(lambda x: x[5], data_features))\n",
        "        df[\"last_word_eq\"] = list(map(lambda x: x[6], data_features))\n",
        "        df[\"first_word_eq\"] = list(map(lambda x: x[7], data_features))\n",
        "        df[\"abs_len_diff\"] = list(map(lambda x: x[8], data_features))\n",
        "        df[\"mean_len\"] = list(map(lambda x: x[9], data_features))\n",
        "\n",
        "        print(\"Extracting Fuzzy Features..\")\n",
        "\n",
        "        df[\"token_set_ratio\"] = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "        df[\"token_sort_ratio\"] = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "        df[\"fuzz_ratio\"] = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "        df[\"fuzz_partial_ratio\"] = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "        df[\"longest_substr_ratio\"] = df.apply(lambda x: self.get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]),\n",
        "                                              axis=1)\n",
        "        return df\n",
        "\n",
        "    def g2w2v(self, list_of_sent, model, d):\n",
        "        sent_vectors = []\n",
        "        for sentence in list_of_sent:\n",
        "            doc = [word for word in sentence if word in model.wv.vocab]\n",
        "            if doc:\n",
        "                sent_vec = np.mean(model.wv[doc], axis=0)\n",
        "            else:\n",
        "                sent_vec = np.zeros(d)\n",
        "            sent_vectors.append(sent_vec)\n",
        "        return sent_vectors\n",
        "\n",
        "    def get_distance_features(self, df):\n",
        "\n",
        "        print(\"Extracting Distance Features..\")\n",
        "\n",
        "        df['question1'] = df.question1.apply(self.remove_stop)\n",
        "        df['question2'] = df.question2.apply(self.remove_stop)\n",
        "        df['word_mover_dist'] = df.apply(\n",
        "            lambda x: self.word_mover_dis(x['question1'], x['question2'], self.glove_model), axis=1)\n",
        "\n",
        "        print(\"- word_mover_dis done...\")\n",
        "\n",
        "        ques1_list = list()\n",
        "        ques2_list = list()\n",
        "\n",
        "        for sentence in df.question1.values:\n",
        "            ques1_list.append(sentence.split())\n",
        "        for sentence in df.question2.values:\n",
        "            ques2_list.append(sentence.split())\n",
        "\n",
        "        g2w2v_ques1 = self.g2w2v(ques1_list, self.glove_model, 300)\n",
        "        g2w2v_ques2 = self.g2w2v(ques2_list, self.glove_model, 300)\n",
        "\n",
        "        print(\"- embedding done...\")\n",
        "\n",
        "        df['cosine_dist'] = [cosine(ques1, ques2) for (ques1, ques2) in zip(g2w2v_ques1, g2w2v_ques2)]\n",
        "        df['cityblock_dist'] = [cityblock(ques1, ques2) for (ques1, ques2) in zip(g2w2v_ques1, g2w2v_ques2)]\n",
        "        df['canberra_dist'] = [canberra(ques1, ques2) for (ques1, ques2) in zip(g2w2v_ques1, g2w2v_ques2)]\n",
        "        df['euclidean_dist'] = [euclidean(ques1, ques2) for (ques1, ques2) in zip(g2w2v_ques1, g2w2v_ques2)]\n",
        "        df['minkowski_dist'] = [minkowski(ques1, ques2) for (ques1, ques2) in zip(g2w2v_ques1, g2w2v_ques2)]\n",
        "\n",
        "        print('- spatial distance done')\n",
        "\n",
        "        df.cosine_dist = df.cosine_dist.fillna(0)\n",
        "        df.word_mover_dist = df.word_mover_dist.apply(lambda wmd: 30 if wmd == np.inf else wmd)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def get_token_features(self, ques1, ques2):\n",
        "        features_list = [0.0] * 10\n",
        "\n",
        "        ques1_tokens = ques1.split()\n",
        "        ques2_tokens = ques2.split()\n",
        "\n",
        "        if len(ques1_tokens) == 0 or len(ques2_tokens) == 0:\n",
        "            return features_list\n",
        "\n",
        "        ques1_words = set([word for word in ques1_tokens if word not in self.STOP_WORDS])\n",
        "        ques2_words = set([word for word in ques2_tokens if word not in self.STOP_WORDS])\n",
        "\n",
        "        ques1_stops = set([word for word in ques1_tokens if word in self.STOP_WORDS])\n",
        "        ques2_stops = set([word for word in ques2_tokens if word in self.STOP_WORDS])\n",
        "\n",
        "        common_word_count = len(ques1_words.intersection(ques2_words))\n",
        "        common_stop_count = len(ques1_stops.intersection(ques2_stops))\n",
        "        common_token_count = len(set(ques1_tokens).intersection(set(ques2_tokens)))\n",
        "\n",
        "        features_list[0] = common_word_count / (min(len(ques1_words), len(ques2_words)) + self.SAFE_DIV)\n",
        "        features_list[1] = common_word_count / (max(len(ques1_words), len(ques2_words)) + self.SAFE_DIV)\n",
        "        features_list[2] = common_stop_count / (min(len(ques1_stops), len(ques2_stops)) + self.SAFE_DIV)\n",
        "        features_list[3] = common_stop_count / (max(len(ques1_stops), len(ques2_stops)) + self.SAFE_DIV)\n",
        "        features_list[4] = common_token_count / (min(len(ques1_tokens), len(ques2_tokens)) + self.SAFE_DIV)\n",
        "        features_list[5] = common_token_count / (max(len(ques1_tokens), len(ques2_tokens)) + self.SAFE_DIV)\n",
        "        features_list[6] = int(ques1_tokens[-1] == ques2_tokens[-1])\n",
        "        features_list[7] = int(ques1_tokens[0] == ques2_tokens[0])\n",
        "        features_list[8] = abs(len(ques1_tokens) - len(ques2_tokens))\n",
        "        features_list[9] = (len(ques1_tokens) + len(ques2_tokens)) / 2\n",
        "\n",
        "        return features_list\n",
        "\n",
        "    def get_longest_substr_ratio(self, a, b):\n",
        "        strs = list(distance.lcsubstrings(a, b))\n",
        "        return 0 if len(strs) == 0 else len(strs[0]) / (min(len(a), len(b)) + 1)\n",
        "\n",
        "    def extract_nlp(self, df):\n",
        "        df = self.extract_features(df)\n",
        "        df = self.get_distance_features(df)\n",
        "        df = df.drop(['question1', 'question2'], axis=1)\n",
        "        return df\n",
        "\n",
        "    def feature_engineering(self, question):\n",
        "        df = self.read_csv(question)\n",
        "        df_tm = self.text_mining(df)\n",
        "        df_2 = self.read_csv(question)\n",
        "        df_nlp = self.extract_nlp(df_2)\n",
        "        df_nlp = df_nlp.drop('answers', axis=1)\n",
        "        df_tm = df_tm.merge(df_nlp, on='test_id', how='left')\n",
        "        return df_tm\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBNOlcNjIdFB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}